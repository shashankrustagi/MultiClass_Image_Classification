{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f1c4be",
   "metadata": {},
   "source": [
    "### LOAD LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.models import Model \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, AveragePooling2D, Flatten,Conv2D,MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization\n",
    "from keras.callbacks import Callback,ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eb84f0",
   "metadata": {},
   "source": [
    "### PROCESS IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28672dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=True, \n",
    "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.2, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip = True,  # randomly flip images\n",
    "        vertical_flip=True)\n",
    "# datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
    "\n",
    "# GIVE THE PATH OF THE TRAIN AND TEST IMAGES \n",
    "train_generator = datagen.flow_from_directory('.../train/images/', target_size=(128,128), class_mode='categorical', batch_size=128)\n",
    "\n",
    "# val_generator = datagen.flow_from_directory('D:/output/val/', target_size=(128,128), class_mode='categorical', batch_size=128)\n",
    "# test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
    "test_generator = datagen.flow_from_directory('.../output/test/', target_size=(128,128), class_mode='categorical', batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6540c72",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a873035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define a Convolutional Neural Network Model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu',\n",
    "                 input_shape = (128,128,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(learning_rate),\n",
    "              metrics=['accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])\n",
    "\n",
    "model.summary()\n",
    "history= model.fit(train_generator,epochs=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65ef446",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE THE RESULTS IN A CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423801b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=model.predict(test_generator, steps=len(test_generator))\n",
    "prediction.shape\n",
    "prediction_index= np.argmax(prediction, axis=1)\n",
    "prediction_index.shape\n",
    "labels=train_generator.class_indices\n",
    "print(type(labels), len(labels))\n",
    "labels = dict((value,key) for key,value in labels.items())\n",
    "# labels\n",
    "predicted_class = [labels[k] for k in prediction_index]\n",
    "len(predicted_class)\n",
    "# predicted_class\n",
    "\n",
    "filenames_ = test_generator.filenames\n",
    "filenames_\n",
    "\n",
    "filenames=[]\n",
    "\n",
    "for e in filenames_:\n",
    "    e = e[7:]\n",
    "    filenames.append(e)\n",
    "#     print(filenames)\n",
    "\n",
    "results = pd.DataFrame({\"file_name\":filenames})\n",
    "results.head()\n",
    "\n",
    "results = pd.DataFrame({\"file_name\":filenames, \"category\":predicted_class})\n",
    "results.head()\n",
    "\n",
    "results.to_csv('model_cnn.csv', index=False)\n",
    "# model.save('cnn.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60814d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('No. of epochs')\n",
    "plt.ylabel('accuracies')\n",
    "plt.legend(['Train','Validation'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('No. of epochs')\n",
    "plt.ylabel('Losses')\n",
    "plt.legend(['Train','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eba8c4",
   "metadata": {},
   "source": [
    "### TUNED CNN (BASE MODEL PERFORM BETTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9388c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu',\n",
    "                 input_shape = (128,128,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(learning_rate),\n",
    "              metrics=['accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])\n",
    "\n",
    "model.summary()\n",
    "history= model.fit(train_generator,epochs=30)\n",
    "prediction=model.predict(test_generator, steps=len(test_generator))\n",
    "prediction.shape\n",
    "prediction_index= np.argmax(prediction, axis=1)\n",
    "prediction_index.shape\n",
    "labels=train_generator.class_indices\n",
    "print(type(labels), len(labels))\n",
    "labels = dict((value,key) for key,value in labels.items())\n",
    "# labels\n",
    "predicted_class = [labels[k] for k in prediction_index]\n",
    "len(predicted_class)\n",
    "# predicted_class\n",
    "\n",
    "filenames_ = test_generator.filenames\n",
    "filenames_\n",
    "\n",
    "filenames=[]\n",
    "\n",
    "for e in filenames_:\n",
    "    e = e[7:]\n",
    "    filenames.append(e)\n",
    "#     print(filenames)\n",
    "\n",
    "results = pd.DataFrame({\"file_name\":filenames})\n",
    "results.head()\n",
    "\n",
    "results = pd.DataFrame({\"file_name\":filenames, \"category\":predicted_class})\n",
    "results.head()\n",
    "\n",
    "results.to_csv('cnn_tuned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
